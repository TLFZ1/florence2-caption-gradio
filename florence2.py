# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------
# å¯¼å…¥æ‰€éœ€çš„åº“
# ----------------------------------------------------------------------
import os  # ç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’ï¼Œå¦‚æ–‡ä»¶è·¯å¾„æ“ä½œ
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import time  # æ—¶é—´ç›¸å…³çš„åŠŸèƒ½
import importlib.metadata  # ç”¨äºæ£€æŸ¥å·²å®‰è£…çš„åŒ…ç‰ˆæœ¬
import re  # æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºæ–‡æœ¬æ¸…ç†
from pathlib import Path  # é¢å‘å¯¹è±¡çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„åº“ï¼Œæ–¹ä¾¿åœ°å¤„ç†æ–‡ä»¶å’Œç›®å½•
from PIL import Image  # Python Imaging Library (PIL) çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œç”¨äºæ‰“å¼€ã€æ“ä½œå’Œä¿å­˜å¤šç§å›¾åƒæ–‡ä»¶æ ¼å¼
from transformers import AutoProcessor, AutoModelForCausalLM  # Hugging Faceçš„transformersåº“ï¼Œç”¨äºåŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
from torch.utils.data import Dataset, DataLoader  # PyTorchçš„æ•°æ®åŠ è½½å·¥å…·
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºç¾è§‚çš„è¿›åº¦æ¡
import gc  # åƒåœ¾å›æ”¶æ¨¡å—ï¼Œç”¨äºæ‰‹åŠ¨ç®¡ç†å†…å­˜
import gradio as gr # å¯¼å…¥Gradioåº“

# ======================================================================
# --- 1. Gradioç•Œé¢é»˜è®¤å€¼é…ç½®åŒº ---
# ======================================================================
# åœ¨è¿™é‡Œä¿®æ”¹å˜é‡ï¼ŒGradioç•Œé¢çš„é»˜è®¤å€¼ä¼šè‡ªåŠ¨æ›´æ–°

# --- è·¯å¾„é…ç½® ---
DEFAULT_IMAGE_FOLDER = r""  # é»˜è®¤çš„è¾“å…¥å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
DEFAULT_OUTPUT_FOLDER = r"" # é»˜è®¤çš„è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
DEFAULT_MODEL_STORAGE_DIR = "./florence2_models"  # æ¨¡å‹ä¸‹è½½å’Œç¼“å­˜çš„æœ¬åœ°ç›®å½•

# --- æ¨¡å‹é…ç½® ---
# å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œç”¨äºGradioä¸‹æ‹‰èœå•
MODEL_ID_LIST = [
    "microsoft/Florence-2-large",
    "microsoft/Florence-2-base",
    "microsoft/Florence-2-large-ft",
    "microsoft/Florence-2-base-ft",
    "MiaoshouAI/Florence-2-large-PromptGen-v2.0"
]
DEFAULT_MODEL_ID = "microsoft/Florence-2-large"  # é»˜è®¤ä½¿ç”¨çš„Hugging Faceæ¨¡å‹ID

# --- åŠŸèƒ½å¼€å…³ ---
DEFAULT_CLEANUP_EXISTING = True  # æ˜¯å¦åœ¨å¼€å§‹æ—¶æ¸…ç†å·²å­˜åœ¨çš„.txtæ–‡ä»¶
DEFAULT_SKIP_EXISTING = True  # æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„.txtæ–‡ä»¶ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ 

# --- æ€§èƒ½ä¸ç²¾åº¦é…ç½® ---
DEFAULT_BATCH_SIZE = 15  # æ‰¹å¤„ç†å¤§å°
DEFAULT_NUM_WORKERS = 6  # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
DEFAULT_ATTN_IMPLEMENTATION = "sdpa"  # æ³¨æ„åŠ›æœºåˆ¶å®ç°
DEFAULT_PRECISION = "fp32"  # è®¡ç®—ç²¾åº¦

# --- ç”Ÿæˆæ•ˆæœé…ç½® ---
DEFAULT_TASK_PROMPT = "<MORE_DETAILED_CAPTION>"  # é»˜è®¤çš„ä»»åŠ¡æç¤ºè¯
DEFAULT_MAX_NEW_TOKENS = 1024  # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
DEFAULT_NUM_BEAMS = 3  # é›†æŸæœç´¢çš„å®½åº¦
DEFAULT_EARLY_STOPPING = False  # æ˜¯å¦å¯ç”¨æå‰åœæ­¢ç­–ç•¥

# --- å…¨å±€å˜é‡ï¼Œç”¨äºç¼“å­˜æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œé¿å…é‡å¤åŠ è½½ ---
loaded_models_cache = {}

# ======================================================================
# --- 2. æ ¸å¿ƒåŠŸèƒ½ä»£ç  ---
# ======================================================================

def print_acceleration_status(device_str, attn_implementation):
    """æ£€æŸ¥å¹¶æ‰“å°å½“å‰PyTorchç¯å¢ƒçš„SDPA/FlashAttentionåŠ é€ŸçŠ¶æ€ã€‚"""
    console_output = "\n--- åŠ é€ŸçŠ¶æ€è¯Šæ–­ ---\n"
    device = torch.device(device_str)
    if device.type != 'cuda':
        console_output += f"ğŸŒ è®¾å¤‡: {device.type.upper()} / åŠ é€Ÿä¸å¯ç”¨\n"
        console_output += "---------------------\n"
        return console_output
    
    console_output += f"è®¾å¤‡: {torch.cuda.get_device_name(0)} (CUDA)\n"
    
    try:
        importlib.metadata.version('flash-attn')
        flash_attn_installed = True
    except importlib.metadata.PackageNotFoundError:
        flash_attn_installed = False

    if attn_implementation == 'flash_attention_2':
        if flash_attn_installed:
            console_output += "âœ… åŠ é€Ÿæ–¹æ¡ˆ: å¼ºåˆ¶ä½¿ç”¨ FlashAttention 2\n"
        else:
            console_output += "âŒ åŠ é€Ÿæ–¹æ¡ˆ: å¼ºåˆ¶ä½¿ç”¨ FlashAttention 2 (ä½†é…ç½®å¯èƒ½å¤±è´¥)\n"
            console_output += "   è­¦å‘Š: æ‚¨æŒ‡å®šä½¿ç”¨ flash_attention_2ï¼Œä½†æœªæ£€æµ‹åˆ° 'flash-attn' åŒ…ï¼\n"
        console_output += "---------------------\n"
        return console_output

    console_output += f"âœ… åŠ é€Ÿæ–¹æ¡ˆ: SDPA (PytorchåŸç”Ÿå®ç°)\n"
    if flash_attn_installed and torch.backends.cuda.flash_sdp_enabled():
        console_output += "   åç«¯å†…æ ¸: âš¡ FlashAttention (å·²å®‰è£…ç‹¬ç«‹çš„ flash-attn åŒ…)\n"
    elif torch.backends.cuda.flash_sdp_enabled():
        console_output += "   åç«¯å†…æ ¸: ğŸ§  PyTorch å†…ç½® FlashAttention\n"
    elif torch.backends.cuda.mem_efficient_sdp_enabled():
        console_output += "   åç«¯å†…æ ¸: ğŸ§  PyTorch å†…ç½®å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆ\n"
    else:
        console_output += "   åç«¯å†…æ ¸: ğŸ§® PyTorch åŸç”Ÿæ•°å­¦å®ç°\n"
    console_output += "---------------------\n"
    return console_output

def cleanup_caption(text: str, task_prompt: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šè¯ï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹åˆå¹¶æˆä¸€ä¸ªå¹²å‡€çš„å•è¡Œå­—ç¬¦ä¸²ã€‚"""
    if not isinstance(text, str):
        return ""
    # å®šä¹‰éœ€è¦ç§»é™¤çš„ç‰¹æ®Šè¯åˆ—è¡¨
    tokens_to_remove = ["<pad>", "</s>", "<s>", task_prompt]
    # å¾ªç¯æ›¿æ¢
    for token in tokens_to_remove:
        text = text.replace(token, "")
    # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    text = text.replace('\r\n', ' ').replace('\n', ' ')
    # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    text = re.sub(r'\s+', ' ', text)
    # è¿”å›æ¸…ç†å¹¶å»é™¤é¦–å°¾ç©ºæ ¼åçš„æ–‡æœ¬
    return text.strip()

def cleanup_existing_files(output_folder, task_prompt, progress=gr.Progress(track_tqdm=True)):
    """éå†è¾“å‡ºæ–‡ä»¶å¤¹ï¼Œæ ¹æ®cleanup_captionçš„è§„åˆ™æ¸…ç†æ‰€æœ‰.txtæ–‡ä»¶ã€‚"""
    output_path = Path(output_folder)
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not output_path.is_dir():
        return "çŠ¶æ€ï¼šè¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†ã€‚", ""
    # æŸ¥æ‰¾æ‰€æœ‰txtæ–‡ä»¶
    txt_files = list(output_path.glob("*.txt"))
    if not txt_files:
        return "çŠ¶æ€ï¼šæœªæ‰¾åˆ°ä»»ä½•.txtæ–‡ä»¶ï¼Œæ— éœ€æ¸…ç†ã€‚", ""
    
    cleaned_count = 0
    console_log = ""
    # ä½¿ç”¨tqdmæ˜¾ç¤ºæ¸…ç†è¿›åº¦
    for file_path in progress.tqdm(txt_files, desc="æ¸…ç†æ—§æ–‡ä»¶"):
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            # æ¸…ç†å†…å®¹
            cleaned_content = cleanup_caption(content, task_prompt)
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œåˆ™å†™å›æ–‡ä»¶
            if content != cleaned_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned_content)
                cleaned_count += 1
                console_log += f"å·²æ¸…ç†: {file_path.name}\n"
        except Exception as e:
            console_log += f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}\n"
    return f"çŠ¶æ€ï¼šæ—§æ–‡ä»¶æ¸…ç†å®Œæˆï¼å…± {cleaned_count} ä¸ªæ–‡ä»¶è¢«æ›´æ–°ã€‚", console_log

class ImageDataset(Dataset):
    """è‡ªå®šä¹‰çš„PyTorchæ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½å›¾ç‰‡ã€‚"""
    def __init__(self, image_paths):
        self.image_paths = image_paths

    def __len__(self):
        # è¿”å›æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°
        return len(self.image_paths)

    def __getitem__(self, idx):
        # æ ¹æ®ç´¢å¼•è·å–å•ä¸ªæ ·æœ¬
        path = self.image_paths[idx]
        try:
            # æ‰“å¼€å›¾ç‰‡å¹¶è½¬æ¢ä¸ºRGBæ ¼å¼
            image = Image.open(path).convert("RGB")
            # è¿”å›å›¾ç‰‡å¯¹è±¡å’Œå…¶è·¯å¾„
            return image, str(path)
        except Exception as e:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ™æ‰“å°é”™è¯¯å¹¶è¿”å›None
            print(f"è­¦å‘Šï¼šåŠ è½½å›¾ç‰‡æ—¶å‡ºé”™ {path}, é”™è¯¯: {e}")
            return None

def collate_fn_skip_none(batch):
    """è‡ªå®šä¹‰çš„collate_fnï¼Œç”¨äºåœ¨DataLoaderç»„åˆæ‰¹æ¬¡æ—¶ï¼Œè¿‡æ»¤æ‰åŠ è½½å¤±è´¥çš„Noneé¡¹ã€‚"""
    return [item for item in batch if item is not None]

def find_image_files(folder_path, recursive):
    """ä½¿ç”¨pathlibæŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ã€‚"""
    dir_path = Path(folder_path)
    if not dir_path.is_dir(): return []
    extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.webp', '.gif']
    glob_pattern = '**/*' if recursive else '*'
    return [p for p in dir_path.glob(glob_pattern) if p.is_file() and p.suffix.lower() in extensions]

# ======================================================================
# --- 3. Gradio æ ¸å¿ƒå¤„ç†å‡½æ•° ---
# ======================================================================

def run_batch_tagging_ui(
    # ä»Gradioç•Œé¢æ¥æ”¶æ‰€æœ‰å‚æ•°
    image_folder, output_folder, model_storage_dir,
    model_id, precision, attn_implementation,
    batch_size, num_workers,
    task_prompt, max_new_tokens, num_beams, early_stopping,
    cleanup_existing, skip_existing,
    progress=gr.Progress(track_tqdm=True) # Gradioè¿›åº¦æ¡å¯¹è±¡
):
    """
    Gradioç•Œé¢ç‚¹å‡»â€œå¼€å§‹â€åæ‰§è¡Œçš„æ ¸å¿ƒå‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼Œä¼šé€æ­¥yieldçŠ¶æ€æ›´æ–°ã€‚
    """
    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç¼“å­˜å’ŒUIè¾“å‡º
    global loaded_models_cache
    status_message = "çŠ¶æ€ï¼šä»»åŠ¡å‡†å¤‡ä¸­..."
    console_output = ""
    yield status_message, console_output

    # å‚æ•°æ ¡éªŒ
    if not image_folder or not os.path.isdir(image_folder):
        raise gr.Error("é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„æ— æ•ˆæˆ–ä¸å­˜åœ¨ã€‚")
    if not output_folder:
        raise gr.Error("é”™è¯¯ï¼šå¿…é¡»æŒ‡å®šè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ã€‚")
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹å¹¶æ‰§è¡Œæ¸…ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
    output_folder_path = Path(output_folder)
    output_folder_path.mkdir(parents=True, exist_ok=True)
    if cleanup_existing:
        status_message, cleanup_log = cleanup_existing_files(output_folder, task_prompt, progress)
        console_output += cleanup_log
        yield status_message, console_output

    # --- æ™ºèƒ½æ¨¡å‹åŠ è½½/å¸è½½é€»è¾‘ ---
    status_message = "çŠ¶æ€ï¼šæ­£åœ¨å‡†å¤‡æ¨¡å‹..."
    yield status_message, console_output
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    console_output += print_acceleration_status(device, attn_implementation)
    yield status_message, console_output

    # ä½¿ç”¨æ¨¡å‹é…ç½®åˆ›å»ºå”¯ä¸€çš„é”®ï¼Œç”¨äºç¼“å­˜
    new_model_key = f"{model_id}-{precision}-{attn_implementation}"
    current_model_key = next(iter(loaded_models_cache)) if loaded_models_cache else None

    # å¦‚æœè¯·æ±‚çš„æ¨¡å‹ä¸å½“å‰åŠ è½½çš„æ¨¡å‹ä¸åŒï¼Œæˆ–è€…æ²¡æœ‰ä»»ä½•æ¨¡å‹è¢«åŠ è½½
    if new_model_key != current_model_key:
        # 1. å¸è½½æ—§æ¨¡å‹ (å¦‚æœå­˜åœ¨)
        if current_model_key is not None:
            console_output += f"\næ­£åœ¨ä»æ˜¾å­˜å¸è½½æ—§æ¨¡å‹: {current_model_key}...\n"
            yield status_message, console_output
            old_model, old_processor = loaded_models_cache.pop(current_model_key)
            del old_model
            del old_processor
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            console_output += "æ—§æ¨¡å‹å·²å¸è½½ã€‚\n"
            yield status_message, console_output
        
        # 2. åŠ è½½æ–°æ¨¡å‹
        try:
            console_output += f"é¦–æ¬¡åŠ è½½æ–°æ¨¡å‹ {model_id}...\n"
            yield status_message, console_output
            torch_dtype = {"bf16": torch.bfloat16, "fp16": torch.float16, "fp32": torch.float32}[precision]
            if precision == "bf16" and device == 'cuda' and not torch.cuda.is_bf16_supported():
                console_output += "è­¦å‘Šï¼šGPUä¸æ”¯æŒbf16ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°fp16ã€‚\n"
                torch_dtype = torch.float16
            
            # ä»Hugging Face HubåŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
            model = AutoModelForCausalLM.from_pretrained(
                model_id, cache_dir=model_storage_dir, attn_implementation=attn_implementation, 
                torch_dtype=torch_dtype, trust_remote_code=True
            ).to(device).eval() # .eval()åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
            processor = AutoProcessor.from_pretrained(model_id, cache_dir=model_storage_dir, trust_remote_code=True)
            
            # 3. å°†æ–°æ¨¡å‹å­˜å…¥ç¼“å­˜
            loaded_models_cache[new_model_key] = (model, processor)
            console_output += "æ–°æ¨¡å‹åŠ è½½æˆåŠŸï¼\n"
            yield status_message, console_output
        except Exception as e:
            raise gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    else:
        # å¦‚æœè¯·æ±‚çš„æ¨¡å‹å·²åŠ è½½ï¼Œç›´æ¥ä½¿ç”¨
        console_output += "ä½¿ç”¨å·²ç¼“å­˜çš„æ¨¡å‹ã€‚\n"
        yield status_message, console_output

    # ä»ç¼“å­˜ä¸­è·å–å½“å‰è¦ä½¿ç”¨çš„æ¨¡å‹
    model, processor = loaded_models_cache[new_model_key]

    # --- æ‰«æå¹¶ç­›é€‰è¦å¤„ç†çš„å›¾ç‰‡æ–‡ä»¶ ---
    status_message = "çŠ¶æ€ï¼šæ­£åœ¨æ‰«æå¹¶ç­›é€‰å›¾ç‰‡..."
    yield status_message, console_output
    
    all_image_paths = find_image_files(image_folder, recursive=True)
    if not all_image_paths:
        yield "å®Œæˆï¼šåœ¨è¾“å…¥æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ã€‚", console_output
        return
        
    if skip_existing:
        # å¦‚æœå¯ç”¨â€œè·³è¿‡â€ï¼Œåˆ™åªä¿ç•™é‚£äº›æ²¡æœ‰å¯¹åº”.txtæ–‡ä»¶çš„å›¾ç‰‡
        unprocessed_images = [p for p in all_image_paths if not (output_folder_path / (p.stem + ".txt")).exists()]
    else:
        unprocessed_images = all_image_paths
            
    total_images = len(all_image_paths)
    processed_count = total_images - len(unprocessed_images)
    
    console_output += f"æ€»å…±æ‰¾åˆ° {total_images} å¼ å›¾ç‰‡ã€‚\n"
    if skip_existing:
        console_output += f"å…¶ä¸­ {processed_count} å¼ å·²æœ‰æ ‡ç­¾ï¼Œå°†è¢«è·³è¿‡ã€‚\n"
    
    if not unprocessed_images:
        yield "å®Œæˆï¼šæ‰€æœ‰å›¾ç‰‡å‡å·²å¤„ç†å®Œæ¯•ï¼", console_output
        return

    console_output += f"å³å°†å¤„ç† {len(unprocessed_images)} å¼ æ–°å›¾ç‰‡...\n"
    yield status_message, console_output

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = ImageDataset(unprocessed_images)
    dataloader = DataLoader(
        dataset, batch_size=int(batch_size), shuffle=False, num_workers=int(num_workers),
        collate_fn=collate_fn_skip_none, pin_memory=True if device == "cuda" else False
    )

    # --- æ‰¹é‡å¤„ç†å¾ªç¯ ---
    start_time = time.time()
    newly_processed_count = 0
    for batch in progress.tqdm(dataloader, desc="æ‰¹é‡å¤„ç†ä¸­"):
        if not batch: continue # å¦‚æœæ‰¹æ¬¡ä¸ºç©ºï¼Œè·³è¿‡
        images, batch_image_paths = zip(*batch)
        try:
            # åœ¨no_gradä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œæ‰€æœ‰PyTorchæ“ä½œï¼Œä»¥é˜²æ­¢æ˜¾å­˜æ³„æ¼
            with torch.no_grad():
                task_prompts = [task_prompt] * len(images)
                inputs = processor(text=task_prompts, images=images, return_tensors="pt").to(device)
                
                # æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
                generated_ids = model.generate(
                    input_ids=inputs["input_ids"], pixel_values=inputs["pixel_values"],
                    max_new_tokens=int(max_new_tokens), num_beams=int(num_beams), early_stopping=early_stopping
                )
                # è§£ç ç”Ÿæˆçš„IDä¸ºæ–‡æœ¬
                generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=False)
            
            # éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªç»“æœ
            for idx, single_text in enumerate(generated_texts):
                current_image = images[idx]
                current_path = Path(batch_image_paths[idx])

                # æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸ºåå¤„ç†å™¨æ·»åŠ image_sizeå‚æ•°
                post_process_args = {'task': task_prompt}
                if "MiaoshouAI" in model_id or "-ft" in model_id:
                    post_process_args['image_size'] = current_image.size

                # åå¤„ç†ç”Ÿæˆç»“æœ
                parsed_answer = processor.post_process_generation(single_text, **post_process_args)
                raw_caption = parsed_answer.get(task_prompt, "")
                # æ¸…ç†æœ€ç»ˆçš„æè¿°æ–‡æœ¬
                caption = cleanup_caption(raw_caption, task_prompt)
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                if caption:
                    output_filename = current_path.stem + ".txt"
                    output_filepath = output_folder_path / output_filename
                    with open(output_filepath, "w", encoding="utf-8") as f:
                        f.write(caption)
                    console_output += f"å·²å¤„ç†: {current_path.name}\n"
                    newly_processed_count += 1
        except Exception as e:
            # æ•è·å¹¶è®°å½•é”™è¯¯
            console_output += f"\né”™è¯¯ï¼šå¤„ç†æ‰¹æ¬¡æ—¶å‘ç”Ÿæ„å¤–: {e}\n"
            import traceback
            console_output += traceback.format_exc() + "\n"
        finally:
            # æ‰‹åŠ¨æ¸…ç†å¾ªç¯ä¸­çš„å˜é‡å’ŒCUDAç¼“å­˜ï¼Œé˜²æ­¢æ˜¾å­˜æ³„æ¼
            del images, batch_image_paths, task_prompts, inputs, generated_ids, generated_texts
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        
        # æµå¼æ›´æ–°UIçŠ¶æ€
        yield f"çŠ¶æ€ï¼šå¤„ç†ä¸­...({newly_processed_count}/{len(unprocessed_images)})", console_output

    # ä»»åŠ¡ç»“æŸï¼Œè®¡ç®—å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    end_time = time.time()
    total_time = end_time - start_time
    images_per_second = newly_processed_count / total_time if total_time > 0 else 0
    
    final_summary = (f"å®Œæˆï¼\næœ¬æ¬¡è¿è¡Œå¤„ç†äº† {newly_processed_count} å¼ æ–°å›¾ç‰‡ã€‚\n"
        f"æ€»è€—æ—¶: {total_time:.2f} ç§’\n"
        f"å¹³å‡é€Ÿåº¦: {images_per_second:.2f} å¼ /ç§’")
    yield final_summary, console_output

# ======================================================================
# --- 4. Gradio ç•Œé¢å®šä¹‰ ---
# ======================================================================

def create_ui():
    """åˆ›å»ºå¹¶å®šä¹‰Gradioç•Œé¢çš„æ‰€æœ‰ç»„ä»¶å’Œå¸ƒå±€ã€‚"""
    # ä½¿ç”¨Gradioçš„é»˜è®¤ä¸»é¢˜ï¼Œæ ¹æ®ç”¨æˆ·åé¦ˆï¼Œè¯¥ä¸»é¢˜åœ¨å…¶ç³»ç»Ÿä¸Šæ˜¾ç¤ºä¸ºæ©™è‰²
    with gr.Blocks(title="Florence-2 æ‰¹é‡æ‰“æ ‡å·¥å…·", theme=gr.themes.Default()) as demo:
        # é¡¶å±‚æ ‡é¢˜
        gr.Markdown("# ğŸ–¼ï¸ Florence-2 æ‰¹é‡å›¾ç‰‡æè¿°ç”Ÿæˆå·¥å…·")
        gr.Markdown("ä¸€ä¸ªåŸºäºFlorence-2ç³»åˆ—æ¨¡å‹çš„é«˜æ€§èƒ½ã€å¯è§†åŒ–çš„æ‰¹é‡å›¾ç‰‡æ‰“æ ‡å·¥å…·ã€‚")

        # ä¸»å¸ƒå±€ï¼Œåˆ†ä¸ºå·¦å³ä¸¤åˆ—
        with gr.Row():
            # å·¦ä¾§ä¸»è®¾ç½®åŒº
            with gr.Column(scale=2):
                # ä½¿ç”¨å¯æŠ˜å çš„Accordionç»„ä»¶æ¥ç»„ç»‡UIï¼Œä½¿å…¶æ›´æ•´æ´
                with gr.Accordion("ä¸»è¦è®¾ç½®", open=True):
                    gr.Markdown("### **1. è·¯å¾„è®¾ç½®**")
                    with gr.Row():
                        image_folder_input = gr.Textbox(label="è¾“å…¥å›¾ç‰‡æ–‡ä»¶å¤¹", value=DEFAULT_IMAGE_FOLDER, placeholder="ä¾‹å¦‚: D:\\dataset\\images")
                        output_folder_input = gr.Textbox(label="è¾“å‡ºæ ‡ç­¾æ–‡ä»¶å¤¹", value=DEFAULT_OUTPUT_FOLDER, placeholder="ä¾‹å¦‚: D:\\dataset\\captions")
                    
                    gr.Markdown("### **2. æ¨¡å‹ä¸ç²¾åº¦**")
                    with gr.Row():
                        model_id_input = gr.Dropdown(MODEL_ID_LIST, value=DEFAULT_MODEL_ID, label="é€‰æ‹©æ¨¡å‹")
                        precision_input = gr.Dropdown(["fp32", "fp16", "bf16"], value=DEFAULT_PRECISION, label="è®¡ç®—ç²¾åº¦")
                    
                    attn_implementation_input = gr.Dropdown(["sdpa", "flash_attention_2", "eager"], value=DEFAULT_ATTN_IMPLEMENTATION, label="æ³¨æ„åŠ›æœºåˆ¶å®ç°", info="æ¨èä½¿ç”¨ 'sdpa'ã€‚")
                    model_storage_dir_input = gr.Textbox(label="æ¨¡å‹ç¼“å­˜ç›®å½•", value=DEFAULT_MODEL_STORAGE_DIR)

                with gr.Accordion("ç”Ÿæˆæ•ˆæœè®¾ç½®", open=True):
                    task_prompt_input = gr.Textbox(label="ä»»åŠ¡æç¤ºè¯ (Task Prompt)", value=DEFAULT_TASK_PROMPT)
                    with gr.Row():
                        max_new_tokens_input = gr.Slider(64, 2048, value=DEFAULT_MAX_NEW_TOKENS, step=64, label="æœ€å¤§ç”Ÿæˆé•¿åº¦")
                        num_beams_input = gr.Slider(1, 10, value=DEFAULT_NUM_BEAMS, step=1, label="é›†æŸæœç´¢å®½åº¦ (Num Beams)")
                    early_stopping_input = gr.Checkbox(label="å¯ç”¨æå‰åœæ­¢ (Early Stopping)", value=DEFAULT_EARLY_STOPPING, info="è¿½æ±‚æ›´ç®€æ´ã€é«˜æ•ˆçš„è¾“å‡ºæ—¶å‹¾é€‰ã€‚")
                
                with gr.Accordion("è¿è¡Œä¸åŠŸèƒ½è®¾ç½®", open=False):
                    with gr.Row():
                        batch_size_input = gr.Slider(1, 64, value=DEFAULT_BATCH_SIZE, step=1, label="æ‰¹å¤„ç†å¤§å° (Batch Size)")
                        num_workers_input = gr.Slider(0, 16, value=DEFAULT_NUM_WORKERS, step=1, label="æ•°æ®åŠ è½½è¿›ç¨‹æ•°", info="é‡è¦ï¼šä¸ºä½¿â€œå¼ºåˆ¶åœæ­¢â€æŒ‰é’®æœ‰æ•ˆï¼Œæ­¤å€¼å¿…é¡»ä¸º 0ã€‚")
                    with gr.Row():
                        cleanup_existing_input = gr.Checkbox(label="å¼€å§‹å‰æ¸…ç†æ—§æ ‡ç­¾æ–‡ä»¶", value=DEFAULT_CLEANUP_EXISTING)
                        skip_existing_input = gr.Checkbox(label="è·³è¿‡å·²å­˜åœ¨çš„æ ‡ç­¾æ–‡ä»¶", value=DEFAULT_SKIP_EXISTING)
                
                # æ§åˆ¶æŒ‰é’®
                with gr.Row():
                    start_button = gr.Button("å¼€å§‹æ‰¹é‡æ‰“æ ‡", variant="primary", size="lg")
                    stop_button = gr.Button("å¼ºåˆ¶åœæ­¢", variant="stop", size="lg", visible=False) # é»˜è®¤ä¸å¯è§

            # å³ä¾§çŠ¶æ€å’Œæ—¥å¿—åŒº
            with gr.Column(scale=1):
                gr.Markdown("### **è¿è¡ŒçŠ¶æ€ä¸æ—¥å¿—**")
                status_output = gr.Textbox(label="å½“å‰çŠ¶æ€", interactive=False, lines=3, max_lines=3)
                console_output = gr.Textbox(label="å¤„ç†æ—¥å¿—", interactive=False, lines=25, max_lines=25, autoscroll=True)

        # --- äº‹ä»¶å¤„ç†é€»è¾‘ ---
        # å°†æ‰€æœ‰è¾“å…¥UIç»„ä»¶æ”¶é›†åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæ–¹ä¾¿ä¼ é€’ç»™æ ¸å¿ƒå¤„ç†å‡½æ•°
        run_inputs = [
            image_folder_input, output_folder_input, model_storage_dir_input,
            model_id_input, precision_input, attn_implementation_input,
            batch_size_input, num_workers_input,
            task_prompt_input, max_new_tokens_input, num_beams_input, early_stopping_input,
            cleanup_existing_input, skip_existing_input
        ]
        
        # å®šä¹‰ç‚¹å‡»â€œå¼€å§‹â€æŒ‰é’®åçš„è¡Œä¸º
        def start_running():
            return {start_button: gr.update(visible=False), stop_button: gr.update(visible=True)}
        
        # å®šä¹‰ä»»åŠ¡ç»“æŸæˆ–æ‰‹åŠ¨åœæ­¢åçš„è¡Œä¸º
        def stop_running(status_message=""):
            update_dict = {start_button: gr.update(visible=True), stop_button: gr.update(visible=False)}
            if status_message:
                update_dict[status_output] = gr.update(value=status_message)
            return update_dict

        # â€œå¼€å§‹â€æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶é“¾
        process_event = start_button.click(
            fn=start_running, outputs=[start_button, stop_button] # ç¬¬ä¸€æ­¥ï¼šåˆ‡æ¢æŒ‰é’®å¯è§æ€§
        ).then(
            fn=run_batch_tagging_ui, inputs=run_inputs, outputs=[status_output, console_output] # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
        ).then(
            fn=lambda: stop_running("ä»»åŠ¡å®Œæˆï¼"), outputs=[start_button, stop_button, status_output] # ç¬¬ä¸‰æ­¥ï¼šä»»åŠ¡ç»“æŸåæ¢å¤æŒ‰é’®
        )
        
        # â€œåœæ­¢â€æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶
        stop_button.click(
            fn=lambda: stop_running("çŠ¶æ€ï¼šä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚"), 
            outputs=[start_button, stop_button, status_output], 
            cancels=[process_event] # å…³é”®ï¼šå–æ¶ˆæ­£åœ¨è¿è¡Œçš„process_eventäº‹ä»¶é“¾
        )
        
        return demo

# ======================================================================
# --- 5. ä¸»ç¨‹åºå…¥å£ ---
# ======================================================================

# ã€æ ¸å¿ƒä¿®æ”¹ã€‘å°†UIåˆ›å»ºå’Œå¯åŠ¨é€»è¾‘åŒ…è£¹åœ¨ if __name__ == "__main__": ä¸­
# è¿™æ˜¯è§£å†³Windowsä¸‹å¤šè¿›ç¨‹é”™è¯¯çš„æ ‡å‡†åšæ³•
if __name__ == "__main__":
    app = create_ui()
    app.launch(inbrowser=True) # inbrowser=True ä¼šè‡ªåŠ¨åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ–°æ ‡ç­¾é¡µ